{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df = yf.download(\"^GSPC\", interval='1d' , start=\"2000-01-01\", end=\"2023-04-30\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date         Open         High          Low    Adj Close      Volume  \\\n",
      "0 2000-01-03  1469.250000  1478.000000  1438.359985  1455.219971   931800000   \n",
      "1 2000-01-04  1455.219971  1455.219971  1397.430054  1399.420044  1009000000   \n",
      "2 2000-01-05  1399.420044  1413.270020  1377.680054  1402.109985  1085500000   \n",
      "3 2000-01-06  1402.109985  1411.900024  1392.099976  1403.449951  1092300000   \n",
      "4 2000-01-07  1403.449951  1441.469971  1400.729980  1441.469971  1225200000   \n",
      "\n",
      "         Close  \n",
      "0  1455.219971  \n",
      "1  1399.420044  \n",
      "2  1402.109985  \n",
      "3  1403.449951  \n",
      "4  1441.469971  \n"
     ]
    }
   ],
   "source": [
    "target = 'Close'\n",
    "\n",
    "df = df.reset_index()\n",
    "df_cols = list(df.columns)\n",
    "df_cols.pop(0)\n",
    "df_cols.insert(0,'date')\n",
    "df = df.set_axis(df_cols,axis=1)\n",
    "df_cols.remove(target)\n",
    "df_cols.append(target)\n",
    "df = df[df_cols]\n",
    "print( df.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(df, time_col = 'date', target = 'Close', features = 'M', also_return_target_scaler = False):\n",
    "    \n",
    "    if features == 'S':\n",
    "        df = df[[time_col, target]]\n",
    "        scaled_df = df.copy()\n",
    "    elif features == 'M' or features == 'MS':\n",
    "        scaled_df = df.copy()\n",
    "        for any_col in df.drop(columns=[target, time_col]).columns:\n",
    "            scaler_ = MinMaxScaler()\n",
    "            scaled_df[any_col] = scaler_.fit_transform( df[any_col].values.reshape((-1,1)) ).reshape(-1)\n",
    "    else:\n",
    "        raise NotImplementedError('NotImplemented!')\n",
    "    \n",
    "    target_scaler = MinMaxScaler()\n",
    "    scaled_df[target] = target_scaler.fit_transform( df[target].values.reshape((-1,1)) ).reshape(-1)\n",
    "    \n",
    "    joblib.dump(target_scaler, 'target_scaler.pkl')\n",
    "    \n",
    "    if also_return_target_scaler:\n",
    "        return scaled_df, target_scaler\n",
    "    else:\n",
    "        return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = scaler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.191497</td>\n",
       "      <td>0.189829</td>\n",
       "      <td>0.187582</td>\n",
       "      <td>0.189001</td>\n",
       "      <td>0.051867</td>\n",
       "      <td>0.189001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0.188096</td>\n",
       "      <td>0.184304</td>\n",
       "      <td>0.177631</td>\n",
       "      <td>0.175457</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>0.175457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>0.174570</td>\n",
       "      <td>0.174130</td>\n",
       "      <td>0.172829</td>\n",
       "      <td>0.176110</td>\n",
       "      <td>0.065713</td>\n",
       "      <td>0.176110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>0.175222</td>\n",
       "      <td>0.173798</td>\n",
       "      <td>0.176335</td>\n",
       "      <td>0.176436</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.176436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>0.175547</td>\n",
       "      <td>0.180969</td>\n",
       "      <td>0.178433</td>\n",
       "      <td>0.185664</td>\n",
       "      <td>0.078299</td>\n",
       "      <td>0.185664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>0.836993</td>\n",
       "      <td>0.836005</td>\n",
       "      <td>0.838991</td>\n",
       "      <td>0.839923</td>\n",
       "      <td>0.264399</td>\n",
       "      <td>0.839923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>0.835626</td>\n",
       "      <td>0.832129</td>\n",
       "      <td>0.827713</td>\n",
       "      <td>0.824047</td>\n",
       "      <td>0.326353</td>\n",
       "      <td>0.824047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>0.826257</td>\n",
       "      <td>0.823214</td>\n",
       "      <td>0.822357</td>\n",
       "      <td>0.820251</td>\n",
       "      <td>0.313595</td>\n",
       "      <td>0.820251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>0.823229</td>\n",
       "      <td>0.834993</td>\n",
       "      <td>0.828663</td>\n",
       "      <td>0.839513</td>\n",
       "      <td>0.305805</td>\n",
       "      <td>0.839513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>0.836402</td>\n",
       "      <td>0.842710</td>\n",
       "      <td>0.841279</td>\n",
       "      <td>0.847797</td>\n",
       "      <td>0.336187</td>\n",
       "      <td>0.847797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5868 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      Open      High       Low  Adj Close    Volume     Close\n",
       "0    2000-01-03  0.191497  0.189829  0.187582   0.189001  0.051867  0.189001\n",
       "1    2000-01-04  0.188096  0.184304  0.177631   0.175457  0.058822  0.175457\n",
       "2    2000-01-05  0.174570  0.174130  0.172829   0.176110  0.065713  0.176110\n",
       "3    2000-01-06  0.175222  0.173798  0.176335   0.176436  0.066326  0.176436\n",
       "4    2000-01-07  0.175547  0.180969  0.178433   0.185664  0.078299  0.185664\n",
       "...         ...       ...       ...       ...        ...       ...       ...\n",
       "5863 2023-04-24  0.836993  0.836005  0.838991   0.839923  0.264399  0.839923\n",
       "5864 2023-04-25  0.835626  0.832129  0.827713   0.824047  0.326353  0.824047\n",
       "5865 2023-04-26  0.826257  0.823214  0.822357   0.820251  0.313595  0.820251\n",
       "5866 2023-04-27  0.823229  0.834993  0.828663   0.839513  0.305805  0.839513\n",
       "5867 2023-04-28  0.836402  0.842710  0.841279   0.847797  0.336187  0.847797\n",
       "\n",
       "[5868 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(df, test_size = 0.2):\n",
    "    \n",
    "    train_size = int( df.shape[0] -(df.shape[0] * test_size) )\n",
    "    \n",
    "    return df.iloc[:train_size], df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df , test_df = splitter(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4694, 7), (1174, 7), (587, 7))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape , test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('dataset_example/WindData/dataset/train/df.csv',index=False)\n",
    "test_df.to_csv('dataset_example/WindData/dataset/test/df.csv',index=False)\n",
    "test_df.to_csv('dataset_example/WindData/dataset/val/df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "is_training = 1\n",
      "model_id = test4\n",
      "model = LSTM\n",
      "plot_flag = 1\n",
      "test_dir = \n",
      "verbose = 1\n",
      "data = Wind\n",
      "root_path = ./dataset_example/WindData/dataset/\n",
      "data_path = df.csv\n",
      "target = Close\n",
      "freq = b\n",
      "checkpoints = ./checkpoints/\n",
      "checkpoint_flag = 1\n",
      "n_closest = None\n",
      "all_stations = 0\n",
      "data_step = 1\n",
      "min_num_nodes = 2\n",
      "features = S\n",
      "seq_len = 5\n",
      "label_len = 1\n",
      "pred_len = 1\n",
      "enc_in = 1\n",
      "dec_in = 1\n",
      "c_out = 1\n",
      "d_model = 8\n",
      "n_heads = 1\n",
      "e_layers = 1\n",
      "d_layers = 1\n",
      "gnn_layers = 1\n",
      "d_ff = 4\n",
      "moving_avg = 25\n",
      "factor = 3\n",
      "distil = True\n",
      "dropout = 0.5\n",
      "embed = timeF\n",
      "activation = gelu\n",
      "output_attention = False\n",
      "win_len = 6\n",
      "res_len = None\n",
      "qk_ker = 4\n",
      "v_conv = 0\n",
      "sparse_flag = 1\n",
      "top_keys = 0\n",
      "kernel_size = 3\n",
      "train_strat_lstm = mixed_teacher_forcing\n",
      "norm_out = 1\n",
      "num_decomp = 4\n",
      "mlp_out = 0\n",
      "num_workers = 0\n",
      "itr = 1\n",
      "train_epochs = 1\n",
      "batch_size = 32\n",
      "patience = 5\n",
      "learning_rate = 0.1\n",
      "lr_decay_rate = 0.8\n",
      "des = test\n",
      "loss = mse\n",
      "lradj = type1\n",
      "use_gpu = False\n",
      "gpu = 0\n",
      "use_multi_gpu = False\n",
      "devices = 0,1,2,3\n",
      "Use CPU\n",
      ">>>>>>>start training : test4_LSTM_Wind_ftS_sl5_ll1_pl1_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 4687\n",
      "val 1167\n",
      "test 1167\n",
      "Could not load best model\n",
      "teacher_forcing_ratio:  0.7200000000000001\n",
      "Updating learning rate to 0.1\n",
      "\titers: 100/146, epoch: 1 | loss: 0.0050458\n",
      "Epoch: 1 cost time: 2.404326915740967\n",
      "Epoch: 1, Steps: 146 | Train Loss: 0.0034743 Test Loss: 0.0071775\n",
      "Test loss decreased (inf --> 0.007177).  Saving model ...\n",
      ">>>>>>>testing : test4_LSTM_Wind_ftS_sl5_ll1_pl1_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 1167\n",
      "loading model\n",
      "test shape: (1152, 1, 1) (1152, 1, 1)\n",
      "test shape: (1152, 1, 1) (1152, 1, 1)\n",
      "mse:0.007065407000482082, mae:0.06500880420207977\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            # status\n",
    "            self.is_training = 1\n",
    "            # model id for saving\n",
    "            self.model_id = 'test4'\n",
    "            # model name\n",
    "            self.model = 'LSTM'\n",
    "            # Whether to save loss plots or not\n",
    "            self.plot_flag = 1\n",
    "            # Base dir to save test results\n",
    "            self.test_dir = ''\n",
    "            # Whether to print inter-epoch losses\n",
    "            self.verbose = 1\n",
    "            # dataset type, Wind or WindGraph\n",
    "            self.data = 'Wind'\n",
    "            # root path of the data file\n",
    "            self.root_path = './dataset_example/WindData/dataset/'\n",
    "            # data file\n",
    "            self.data_path = 'df.csv'\n",
    "            # optional target station for non-graph models\n",
    "            self.target = 'Close'\n",
    "            # freq for time features encoding\n",
    "            self.freq = 'b'\n",
    "            # location of model checkpoints\n",
    "            self.checkpoints = './checkpoints/'\n",
    "            # Whether to checkpoint or not\n",
    "            self.checkpoint_flag = 1\n",
    "            # number of closest nodes for graph connectivity, None --> complete graph\n",
    "            self.n_closest = None\n",
    "            # Whether to use all stations or just target for non-spatial models\n",
    "            self.all_stations = 0\n",
    "            # Only use every nth point. Set data_step = 1 for full dataset\n",
    "            self.data_step = 1\n",
    "            # Minimum number of nodes in a graph\n",
    "            self.min_num_nodes = 2\n",
    "            # forecasting task, options:[M, S]; M:multivariate input, S:univariate input\n",
    "            self.features = 'S'\n",
    "            # input sequence length\n",
    "            self.seq_len = 5\n",
    "            # start token length. Note that Graph models only use label_len and pred_len\n",
    "            self.label_len = 1\n",
    "            # prediction sequence length\n",
    "            self.pred_len = 1\n",
    "            # Number of encoder input features\n",
    "            self.enc_in = 1\n",
    "            # Number of decoder input features\n",
    "            self.dec_in = 1\n",
    "            # output size, note that it is assumed that the target features are placed last\n",
    "            self.c_out = 1\n",
    "            # dimension of model\n",
    "            self.d_model = 512\n",
    "            # num of heads\n",
    "            self.n_heads = 8\n",
    "            # number of encoder layers for non-spatial and number of LSTM or MLP layers for GraphLSTM and GraphMLP\n",
    "            self.e_layers = 4\n",
    "            # num of decoder layers\n",
    "            self.d_layers = 4\n",
    "            # Number of sequential graph blocks in GNN\n",
    "            self.gnn_layers = 2\n",
    "            # dimension of fcn\n",
    "            self.d_ff = 2048\n",
    "            # window size of moving average for Autoformer\n",
    "            self.moving_avg = 25\n",
    "            # attn factor\n",
    "            self.factor = 3\n",
    "            # whether to use distilling in encoder\n",
    "            self.distil = True\n",
    "            # dropout\n",
    "            self.dropout = 0.5\n",
    "            # time features encoding, options:[timeF, fixed, learned]\n",
    "            self.embed = 'timeF'\n",
    "            # activation\n",
    "            self.activation = 'gelu'\n",
    "            # whether to output attention in ecoder\n",
    "            self.output_attention = False\n",
    "            # Local attention length for LogSparse Transformer\n",
    "            self.win_len = 6\n",
    "            # Restart attention length for LogSparse Transformer\n",
    "            self.res_len = None\n",
    "            # Key/Query convolution kernel length for LogSparse Transformer\n",
    "            self.qk_ker = 4\n",
    "            # Weather to apply ConvAttn for values (in addition to K/Q for LogSparseAttn)\n",
    "            self.v_conv = 0\n",
    "            # Weather to apply logsparse mask for LogSparse Transformer\n",
    "            self.sparse_flag = 1\n",
    "            # Weather to find top keys instead of queries in Informer\n",
    "            self.top_keys = 0\n",
    "            # Kernel size for the 1DConv value embedding\n",
    "            self.kernel_size = 3\n",
    "            # The training strategy to use for the LSTM model. recursive or mixed_teacher_forcing\n",
    "            self.train_strat_lstm = 'mixed_teacher_forcing'\n",
    "            # Whether to apply laynorm to outputs of Enc or Dec in FFTransformer\n",
    "            self.norm_out = 1\n",
    "            # Number of wavelet decompositions for FFTransformer\n",
    "            self.num_decomp = 4\n",
    "            # Whether to apply MLP to GNN outputs\n",
    "            self.mlp_out = 0\n",
    "            # data loader num workers\n",
    "            self.num_workers = 0\n",
    "            # experiments times\n",
    "            self.itr = 1\n",
    "            # train epochs\n",
    "            self.train_epochs = 1\n",
    "            # batch size of train input data\n",
    "            self.batch_size = 32\n",
    "            # early stopping patience\n",
    "            self.patience = 5\n",
    "            # optimizer learning rate\n",
    "            self.learning_rate = 0.1\n",
    "            # Rate for which to decay lr with\n",
    "            self.lr_decay_rate = 0.8\n",
    "            # exp description\n",
    "            self.des = 'test'\n",
    "            # loss function\n",
    "            self.loss = 'mse'\n",
    "            # adjust learning rate\n",
    "            self.lradj = 'type1'\n",
    "            # use gpu\n",
    "            self.use_gpu = torch.cuda.is_available()\n",
    "            # gpu\n",
    "            self.gpu = 0\n",
    "            # use multiple gpus, still experimental for graph data\n",
    "            self.use_multi_gpu = False\n",
    "            # device ids of multiple gpus\n",
    "            self.devices = '0,1,2,3'\n",
    "        def __repr__(self):\n",
    "            return '\\n'.join([f'{key} = {value}' for key, value in self.__dict__.items()])\n",
    "    \n",
    "    args = Args()\n",
    "    \n",
    "    # Setup for multi-GPU if specified\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "    \n",
    "    print('Args in experiment:')\n",
    "    print(args)\n",
    "    \n",
    "    Exp = Exp_Main\n",
    "    \n",
    "    if args.is_training:\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_{}'.format(\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.label_len,\n",
    "                args.pred_len,\n",
    "                ii)\n",
    "            \n",
    "            exp = Exp(args)  # set experiments\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "            \n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting, base_dir=args.test_dir)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            ii)\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, base_dir=args.test_dir)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
